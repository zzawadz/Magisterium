\documentclass[a4paper,12pt,openany, DIV=calc, headsepline]{scrbook}
\usepackage[utf8]{inputenc}
%\usepackage[cp1250]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
%\usepackage[fixlanguage]{babelbib}
%\selectbiblanguage{polish}
%\usepackage[T1]{polski}
%\usepackage{times}
\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{lmodern}
%\usepackage{scrpage2}
\usepackage{natbib}
\usepackage{float}
\usepackage{algorithm2e}
\usepackage{longtable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
%\usepackage{subfigure}
%\usepackage{longtable}
%\usepackage{stmaryrd}
%\usepackage{wasysym}
%\usepackage{lscape}
\usepackage{url}
%\usepackage{calc}
%\usepackage{multirow}
%\usepackage{multicol}
\usepackage{enumerate}
\usepackage{makeidx}
%\usepackage{epstopdf}
\usepackage{setspace}
\usepackage{listings}
%\usepackage[usenames,dvipsnames]{pstricks}
%\usepackage{pst-eps} % For gradients
%\usepackage{pst-plot} % For axes
%



\newgeometry{tmargin=2.5cm, bmargin=2.5cm, lmargin=2cm, rmargin=2cm}
\doublespacing
\addcontentsline{toc}{chapter}{Wstęp}
%\singlespacing
\begin{titlepage}
\titlehead{\center{\LARGE Uniwersytet Ekonomiczny w Krakowie}\\
\vskip2mm
{\LARGE Wydział Zarządzania}\\
\vskip2mm
{\LARGE Katedra Statystyki}}

\author{{\LARGE \textbf{Zygmunt Zawadzki}}\\
numer albumu: 161509\\
Kierunek: Analityka Gospodarcza\\
Specjalność: Modelowanie i Prognozowanie}

\subject{Praca magisterska}
\title{Filtracja danych wysokiej częstotliwości}
%\dedication{Rodzicom}
%\subtitle{Short but sweet?}
\publishers{{\Large Opiekun naukowy: prof. UEK dr hab. Daniel Kosiorowski}}
\vskip2mm
\date{Kraków, 2015}
\end{titlepage}
\begin{document}

\maketitle

\thispagestyle{empty}
    \null\vspace{\stretch {1}}
        %\begin{flushright}
        \large
                Powstanie tej pracy nie byłoby możliwe bez zbiorów danych finansowych udostępnionych przez fundusz inwestycyjny Metrum Capital. Szczególne podziękowania należą się Michałowi Pruskiemu, Przemysławowi Sępowi i Annie Homie za wszelką pomoc udzieloną w przygotowaniu danych do analizy. 
        %\end{flushright}
\vspace{\stretch{2}}\null



\tableofcontents
\chapter*{Wstęp}


W ostatnich latach coraz szerszą popularność zdobywają algorytmiczne fundusze inwestycyjne, w których decyzje o inwestycjach podejmowane są z wyłączeniem czynnika ludzkiego. Zastosowanie algorytmów pozwala znacząco skrócić czas analizy potrzebnej do podjęcia decyzji o zajęciu określonej pozycji rynkowej, dzięki czemu w danej chwili może być analizowany dużo szerszy portfel aktywów z wykorzystaniem bardziej złożonych modeli. 

Jednocześnie podejście algorytmiczne krytycznie zależy od jakości dostępnych danych. Ma tu zastosowanie zasada GIGO\footnote{GIGO - Garbage in-Garbage out} mówiąca, że nawet jeśli cały proces analizy jest poprawny, to przy danych złej jakości jakiekolwiek wnioski nie mają sensu. W przypadku gdy inwestycji dokonuje człowiek, niezależnie od tego czy wspomaga się w procesie decyzyjnym analizą fundamentalną, techniczną, czy modelami statystycznymi, może on dokonać oceny jakości danych i dokonać ewentualnego czyszczenia. Poprzez usunięcie obserwacji odstających, analityk zabezpiecza się przed błędni wskazaniami użytych procedur. W przypadku algorytmicznym, z uwagi na ilość analizowanych instrumentów finansowych, jakakolwiek manualna ingerencja w dane jest znacząco utrudniona, bądź w ogóle nie możliwa. W takim przypadku jednym z rozwiązań jest wykorzystanie procedur odpornych, bądź odpowiednich algorytmów usuwania obserwacji odstających.

Najważniejszym celem pracy jest prezentacja metody przygotowywania rzetelnych danych finansowych, które później mogą być użyte w algorytmicznych strategiach inwestycyjnych, począwszy od etapu budowy, poprzez weryfikację, aż do rzeczywistego użycia na rynku. Główny nacisk rozważań zostanie położony na dane wysokiej częstotliwości, na których opiera się prezentowana metoda. Następnie przedstawione zostaną argumenty dotyczące występowania w danych finansowych obserwacji odstających, jak również rola statystyki odpornej w tej dziedzinie. Głównym celem pracy będzie prezentacja filtra pozwalającego usuwać obserwacje odstające w danych wysokiej częstotliwości.

Jednocześnie w pracy poruszone zostaną pewne aspekty związane z prowadzeniem badań w zakresie algorytmicznych strategii inwestycyjnych. Zaprezentowane zostanie pojęcie \emph{backtestu}, będącego podstawowym narzędziem wykorzystywanym do oceny strategii inwestycyjnych. Sam backtest jest bardzo potężnym i złożonym narzędziem, które niestety w bardzo prosty sposób może zostać wykorzystane niepoprawnie, dając mylne wyniki. Dlatego też w pracy położono nacisk na przedstawienie możliwych konsekwencji popełnionych błędów i nieuwzględnienia pewnych zjawisk na otrzymane z backtestu wyniki. Tym samym praca ma na celu ulepszenie wypracowanych standardów prowadzenia badań strategii algorytmicznych w duchu pracy \citep{de2015future}.


\chapter{Wprowadzenie}

\section{Ocena jakość strategii inwestycyjnej}

Symulacja historyczna (ang. \emph{backtest}) jest to  podstawowa procedura umożliwiająca ocenę jakości algorytmicznej strategii inwestycyjnej, bez której ciężko wyobrazić sobie możliwość prowadzenia badań w zakresie algorytmicznych strategii inwestycyjnych. W backteście, na podstawie danych historycznych generowane są hipotetyczne momenty otwarcia i zamknięcia pozycji, a następnie na ich podstawie wyznaczany jest szereg zwrotów dla strategii. Taki szereg w dalszej części służy do weryfikacji działania określonej strategii inwestycyjnej pod wieloma względami (stosunek ryzyka do hipotetycznego zwrotu, maksymalne obsunięcia kapitału, itd.). 

Sam backtest jest bardzo złożonym i wieloetapowym procesem, w którym błąd na jakimkolwiek etapie sprawi, iż otrzymane wnioski będą fałszywe . Co więcej - specyfika popełnianych błędów sprawia, że dużo częściej otrzymuje się wyniki zbyt optymistyczne, przez co twórca strategii ma wrażenie, iż w przyszłości będzie ona zyskowna, gdy faktycznie strategia może niczym nie różnić się od losowego podejmowania decyzji. W dalszej części omówiono kilka podstawowych zagadnień związanych z prowadzeniem rzetelnych symulacji, jednak w żaden sposób nie wyczerpują one listy problemów, które muszą zostać rozwiązane by otrzymane wyniki były wiarygodne.

Jednym z typowych błędów jest nieuwzględnienie w analizie kosztów transakcyjnych (od każdej dokonywanej transakcji giełdowej pobierana jest pewna prowizja) jak również ewentualnego wpływu zlecenia na rynek. Dla przykładu - przy bardzo małej płynności (małej ilości zawieranych transakcji) możliwe jest że w danej chwili cena aktywa na rynku wynosiła $k$\$ i w danych historycznych zarejestrowana jest transakcja po tej cenie, jednak testowany system faktycznie nie byłby w stanie dokonać takiej transakcji (np. nie była dostępna odpowiednia ilość ofert po stronie przeciwnej). Uzyskiwane różnice pomiędzy hipotetyczną ceną wykonania zlecenia a faktyczną, mimo iż bardzo małe\footnote{Prowizja za jedną akcję na amerykańskich giełdach wynosi 0.005USD (\url{https://www.interactivebrokers.com/en/?f=commission&p=stocks1})}, nawarstwiają się, co jest szczególnie widoczne w przypadku strategii wykonujących dużą ilość transakcji.

Jednocześnie dużo poważniejsze błędy dotyczą aspektów statystycznych, a szczególnie zagadnienia \emph{overfittingu}. W przypadku danych finansowych (jak również większości danych ekonomicznych), analityk nie ma możliwości przeprowadzenia eksperymentu - musi więc bazować na dostępnych danych. Z tego też powodu na etapie testowania hipotez dotyczących strategii inwestycyjnych analityk staje przed problemem testowania wielokrotnego. Co więcej, można pokazać (\citep{bailey2013pseudo}), że dla wielu strategii poprzez zwykłą optymalizację można znaleźć zestaw parametrów dających bardzo optymistyczne wyniki, które jednak nie są odtwarzalne w rzeczywistym handlu.

Szerszy przegląd zagadnień związanych z prowadzeniem backtestów można znaleźć w pracach \citep{lopez2015backtesting}, \citep{bailey2014statistical}, informacje na temat sposobów przeciwdziałania overfittingowi \citep{bailey2014probability}, jak również pewnych metod pozwalających prowadzić badania w zakresie strategii inwestycyjnych bez wykorzystywania backtestu - \citep{carr2014determining}.


\section{Książka zleceń}


Kluczową konstrukcją w handlu na rynku finansowym jest książka zleceń. Zawiera ona wszystkie aktualne oferty kupna (Bid) i sprzedaży (Ask), dla danego instrumentu finansowego. W praktyce nazwy \emph{Bid} i \emph{Ask} używane są do określenia najlepszych cen kupna i sprzedaży. Sama książka zleceń składa się z poziomów, na których poszczególne zlecenia uporządkowane są malejąco dla ofert kupna i rosnąco dla ofert sprzedaży. W przypadku gdy pojawia się zlecenie kupna którego cena wykonania jest większa bądź równa cenie dla dostępnych zleceń sprzedaży dochodzi do zawarcia transakcji. Analogicznie dla zlecenia sprzedaży, by doszło do transakcji cena zlecenia musi być niższa, bądź równa cenie dostępnych zleceń kupna. Jeżeli pojawia się nowe zlecenie, które nie prowadzi do zawarcia transakcji trafia ono na odpowiedni poziom książki zleceń. Przykład możliwego wyglądu książki zleceń zaprezentowano na listingu \ref{lst:exOb}.

W pewnych zastosowaniach prowadzi się analizę głębokości (liczby poziomów) książki zleceń. Taka analiza pozwala określić płynność dla danego instrumentu i ocenić wpływ jaki może mieć wprowadzenie zlecenia określonej wielkości. Dla przykładu - jeżeli na pierwszych dziesięciu poziomach znajdują się zlecenia o wolumenie jeden (oferta dotyczy jednej jednostki określonego instrumentu finansowego), wtedy by zawrzeć transakcję opiewającą na 10 jednostek instrumentu finansowego cena zlecenia będzie musiała być co najmniej równa cenie z dziesiątego poziomu. Jednocześnie należy pamiętać, iż w niższych poziomach cena może znacząco odbiegać od aktualnej najlepszej dostępnej ceny na rynku.

W wielu praktycznych sytuacjach jako cenę instrumentu przyjmuje się wartość najwyższej oferty kupna i najniższej oferty sprzedaży. Na ich podstawie wprowadza się następujące pojęcia, które będą używane w dalszej części pracy:

\begin{itemize}
\item \textbf{Bid} - najwyższa aktualnie dostępna cena kupna.
\item \textbf{Ask} - najniższa dostępna cena sprzedaży.
\item \textbf{Mid} - średnia cena instrumentu definiowana jako: (Ask+Bid)/2.
\item \textbf{Spread} - różnica pomiędzy Ask i Bid.
\end{itemize}

Na potrzeby pracy wprowadzone zostały również następujące określenia:
\begin{itemize}
\item \textbf{Tick} - wystąpienie zdarzenia na książce zleceń, będącego zmianą Bid, Ask, lub zawarciem transakcji.
\item \textbf{BidTick} - zmiana ceny Bid.
\item \textbf{AskTick} - zmiana ceny Ask.
\item \textbf{TransTick} - wystąpienie transakcji.
\end{itemize}

W niniejszej pracy analiza została ograniczona jedynie do pierwszego poziomu książki zleceń (Bid i Ask). Zlecenia modyfikujące niższe poziomy są pomijane.

\begin{lstlisting}[float, caption = {Przykład książki zleceń. Opracowano na podstawie pakietu \emph{orderbook} (\citep{orderbookR}) środowiska R}, label={lst:exOb}]
		 Price 	 Ask Size
---------------------------------------------
		 11.42 	 900   
		 11.41 	 1,400 
		 11.40 	 1,205 
		 11.39 	 1,600 
		 11.38 	 400   # pierwszy poziom Ask
---------------------------------------------
  2,700 	 11.36         # pierwszy poziom Bid
  1,100 	 11.35 
  1,100 	 11.34 
  1,600 	 11.33 
    700 	 11.32 
---------------------------------------------
Bid Size 	 Price
\end{lstlisting}

\section{Źródło finansowych danych wysokiej częstotliwości}

Finansowe dane wysokiej częstotliwości  - \emph{HFD} (High-frequency Financial Data - ang. finansowe dane wysokiej częstotliwości) kojarzone są głównie z handlem wysokiej częstotliwości (HFT - ang. High-frequency Trading), w którym pozycje trzymane są w bardzo krótkich interwałach czasowych, często nieprzekraczających jednej sekundy. W takim przypadku zwrot z pojedynczej transakcji jest bardzo niski (opiera się na zmianie ceny w okresie co najwyżej kilku sekund). Z tego powodu inwestor stosujący tego typu podejście, by osiągnąć wysoką rentowność musi dokonywać bardzo wielu tego typu transakcji, co łączy się z bardzo dużą ilością składanych zleceń. Same finansowe dane wysokiej częstotliwości to właśnie informacje o każdym pojawiającym się na rynku zleceniu kupna i sprzedaży. 

Pojawienie się terminu ''finansowe dane wysokiej częstotliwości'' związane jest jednak nie z działaniem funduszy HFT, tylko wzrostu zainteresowania rynkiem finansowym, jak również jego komputeryzacją. Sam wzrost liczby inwestorów spowodował zwiększenie ilości składanych zleceń, a przez to coraz większy obrót. Jednocześnie komputeryzacja giełd pozwoliła sprostać rosnącej aktywności inwestorów, otwierając nowe możliwości osiągnięcia zysku, jak również zmniejszenia kosztów. Jednym z przykładów takiego działania mogła być próba automatycznego wychwytywania pojawiających się zleceń o dużym wolumenie (np. składanych przez banki inwestycyjne), tak by maksymalnie zmienić cenę na niekorzyść zlecającego. By chronić się przed takim działaniem stworzono specjalne strategie pozwalające rozbić jedno duże zlecenie na wiele mniejszych, które w ogólnym wolumenie handlu nie zostaną zauważone. Tym samym zlecenie, które kiedyś było pojedynczą obserwacją w obecnej chwili rozbijane jest na setki dużo mniejszych zleceń, co w prosty sposób przenosi się na wzrost ogólnej ilości danych finansowych. Jednocześnie w dalszym ciągu pojawiają się pewne nieefektywności rynkowe spowodowane pojawiającymi się zleceniami, jednak odnoszą się one do dużo mniejszych wolumenów. Wykorzystaniem tego typu nieefektywności zajmują się właśnie fundusze HFT. O ich znaczeniu świadczy fakt, iż w roku 2012 około 25\% zleceń pojawiających się na światowych giełdach związana była z działalnością tego typu funduszy. Więcej informacji na temat historii giełd oraz rozwoju HFD i HFT można znaleźć w pracy \citep{irene2013}.

\section{Metody przygotowywania danych do testów}

Zastosowanie finansowych danych wysokiej częstotliwości wymaga posiadania odpowiedniej infrastruktury potrzebnej do ich przechowywania i analizy. Dla przykładu jeden miesiąc HFD dla kontraktu ESH15 zajmuje w postaci binarnej około 600 megabajtów, w wersji skompresowanej jest to ok. 60 megabajtów. Należy nadmienić, że ilość instrumentów finansowych może być liczona w dziesiątkach tysięcy.

Jednocześnie, w wielu zastosowaniach taka rozdzielczość danych nie jest w ogóle wymagana, a nawet może być szkodliwa \citep{doman2009modelowanie}. Związane jest to między innymi z efektami mikrostruktury rynku takimi jak ''bid-ask bounce''\footnote{Efekt  ''bid-ask bounce'' związany jest z ''odbijaniem się'' ceny pomiędzy ceną kupna a ceną sprzedaży, powodując ciągłe zmiany ceny transakcyjnej, co w przypadku estymowania zmienności może prowadzić do jej zawyżenia (ruch na instrumencie nie wynika z realnych przesłanek, jest jedynie efektem konstrukcji książki zleceń).}. Dlatego też w praktyce w analizie cen instrumentów finansowych wykorzystywane są świeczki OHLCV (Open, High, Low, Close, Volume). Konstrukcja świeczki w klasycznym podejściu opiera się na agregacji wszystkich transakcji z okresu $t + \delta t$, gdzie $\delta$ określa przedział czasowy świeczki (np. godzina, 10 minut, etc.). Następnie pierwsza dostępna cena określa cenę otwarcia (Open), wartość najwyższa to High, najniższa Low, a ostatnia cena określa cenę zamknięcia (Close), zsumowany wolumen transakcji stanowi Volume. Tak przygotowane dane wykorzystywane są w dalszych etapach analizy finansowej (estymacja modeli zmienności, analizy technicznej i statystycznej etc.). Co więcej, dane w tej postaci są powszechnie dostępne w popularnych serwisach finansowych w internecie.

Przedstawione podejście, oparte jedynie na wykonanych zleceniach, znajduje zastosowanie w przypadku płynnych aktywów, w których w danym okresie czasu dokonuje się wiele transakcji. Jednakże może dojść do sytuacji w której w przedziale $\delta$ nie są zawierane żadne transakcje. Przykład takiego zdarzenia prezentowany jest na rysunku \ref{fig:cottonTrans}. W godzinach od drugiej do trzynastej nie wystąpiły na przedstawionym instrumencie żadne transakcje. W przypadku próby budowy świeczek godzinowych w tym okresie wystąpiłyby brakujące dane.

Brak aktywności transakcyjnej nie oznacza jednak braku aktywności w ogóle. W dalszym ciągu na giełdę mogą przybywać zlecenia trafiające do książki zleceń, zmieniając tym samym rynkową wycenę instrumentu. Dlatego też wprowadza się metodę budowy świeczek OHLCV opartą na aktualnym stanie książki zleceń. W takim przypadku jako cenę otwarcia dla instrumentu, przyjmuje się wartość Mid w chwili $t$, następnie w okresie $<t,t + \delta t>$ monitoruje się wszystkie zmiany Ask i Bid, w celu znalezienia najwyższej i najniższej wartości Mid. Ceną zamknięcia jest Mid w chwili $t + \delta t$. Takie podejście pozwala lepiej uchwycić aktywność rynkową, co prezentuje rysunek \ref{fig:cottonOrder} przedstawiający BidTicki i AskTicki dla tego samego okresu który został przedstawiony na rysunku \ref{fig:cottonTrans}. Wyraźnie widać, że w okresie w którym nie występowały transakcje instrument w dalszym ciągu był aktywny i jego wycena rynkowa zmieniała się. Taka procedura budowy świeczek OHLCV pozwala znacząco zredukować ilość brakujących danych występujących w szeregu czasowym. 


\begin{figure}
  \centering
  \includegraphics[scale=0.5]{wykresy/cottonTickH.PNG}
  \caption{Transakcje na marcowym kontrakcie terminowym na bawełnę w 2015 roku. Przez większą część sesji nie pojawiają się żadne obserwacje.}
  \label{fig:cottonTrans}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{wykresy/cottonOrderH.PNG}
  \caption{Aktywność na pierwszym poziomie książki zleceń na marcowy kontrakt na bawełnę w 2015 roku. Mimo, iż przez większą część sesji nie pojawiają się transakcje, książka zleceń jest aktywna i ceny Bid i Ask cały czas ewokują.}
  \label{fig:cottonOrder}
\end{figure}

\subsection{Przeprowadzanie symulacji historycznych z wykorzystaniem danych OHLCV}


By zapewnić rzetelne wyniki backtestu, dla każdej świeczki OHLCV powinny zostać stworzone towarzyszące świeczki zbudowane z wykorzystaniem cen Bid i Ask. Zostaną one użyte w symulacji historycznej do otwierania i zamykania pozycji. Ma to szczególne znaczenie w przypadku mniej płynnych aktywów, w których spread pomiędzy Bidem i Askiem może być bardzo znaczący - na rysunku \ref{fig:cottonOrder} w pewnych momentach stosunek spreadu do ceny Mid wynosi $0.7\%$, co oznacza, że cena wykonania zlecania kupna byłaby niedoszacowana o ok $0.35\%$, natomiast cena wykonania zleceń sprzedaży byłaby przeszacowana o tę wartość. W takiej sytuacji w testach pozycje zajmowane byłyby po bardziej atrakcyjnych cenach niż były dostępne w rzeczywistości, a uzyskany wynik byłby przeszacowany. Ma to szczególne znaczenie w przypadku strategii o dużej częstotliwości transakcyjnej, w których efekt wykorzystania cen Mid (lub cen wyznaczonych na podstawie transakcji) jest dodatkowo potęgowany przez ilość transakcji. W skrajnych przypadkach cały zysk określonej strategii inwestycyjnej może wynikać jedynie z tego efektu. Dlatego też należy z dużą uwagą podchodzić do wyników algorytmicznych strategii inwestycyjnych przedstawionych w literaturze, gdyż prezentowane w nich wyniki mogą nie uwzględniać opisanego efektu, a co za tym idzie, wyniki tych strategii są one niemożliwe do odtworzenia w warunkach wykorzystujących ceny Bid i Ask. 

Należy zaznaczyć, że dane wysokiej częstotliwości, w oparciu o które powinny być budowane szeregi czasowe dla backtestów, są dostępne jedynie odpłatnie, co znacząco podnosi próg wejścia dla algorytmicznych funduszy inwestycyjnych. Jednocześnie dla pewnej grupy najbardziej płynnych instrumentów finansowych i określonej klasy strategii inwestycyjnych charakteryzujących się niską częstotliwością\footnote{Im większa częstotliwość zawierania transakcji, tym większe znaczenie mają koszty transakcyjne, a nawet minimalne różnice związane ze zmianą ceny w czasie od momentu złożenia zlecenia aż do momentu wykonania.} zawierania transakcji przedstawiony efekt może być bez znaczenia.

\subsection{Czyszczenie danych historycznych.}

W przypadku posiadania bazy danych historycznych istnieje możliwość oczyszczenia istniejących danych historycznych. W takiej sytuacji mogą zostać wykorzystane bardziej zaawansowane techniki wykrywania obserwacji odstających (pojedyncze obserwacje mogą być rozpatrywane względem przeszłych jak i przyszłych danych, por. Rys. \ref{fig:odst1} i Rys. \ref{fig:odst2}). Takie postępowanie może okazać się niepoprawne, gdyż otrzymany szereg może być znacznie lepiej oczyszczony, a przez to mieć inne charakterystyki niż szereg na którym rzeczywiście będzie pracować strategia. W takim przypadku może się okazać, iż przetestowana strategia bardzo dobrze sprawdza się na ''czystych'' danych, a nie zostanie wykryte, iż jakiekolwiek obserwacje odstające drastycznie zmieniają otrzymane wyniki. Z tego też powodu, istotnym może okazać się przeprowadzenie analizy wpływu jakości danych na wskazania strategii. W konsekwencji pewne instrumenty mogą zostać nawet wykluczone z portfela, gdyż dostępne dla nich dane są zbyt zanieczyszczone.  

\begin{figure}
  \centering
  \includegraphics[width=130mm, height=70mm]{wykresy/odst1}
  \caption{Obserwacja czerwona w stosunku do reszty obserwacji (zarówno przeszłych i przyszłych) jest obserwacją odstającą.}
  \label{fig:odst1}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=130mm, height=70mm]{wykresy/odst2}
  \caption{Obserwacja czerwona jest początkiem nowego reżimu danych, dlatego też względem obserwacji przeszłych mogłaby być traktowana jako odstająca. Jednak w świetle dalszych obserwacji nie powinna zostać uznana za odstającą.}
  \label{fig:odst2}
\end{figure}


\section{Charakterystyka danych wysokiej częstotliwości - stylizowane fakty}

Dane wysokiej częstotliwości charakteryzują się innymi własnościami niż klasycznie prezentowane charakterystyki finansowych szeregów czasowych. Co więcej, celem tej pracy jest wykorzystanie danych wysokiej częstotliwości do konstrukcji rzetelnych danych niższej częstotliwości, które będą użyteczne w dalszym modelowaniu statystycznym. Dlatego też pewne stylizowane fakty dotyczące danych finansowych, takie jak grube ogony, czy grupowanie zmienności w przypadku danych wysokiej częstotliwości nie odgrywają tak istotnej roli.

By lepiej przedstawić charakterystykę HFD prezentowane są pewne kluczowe stylizowane fakty, w oparciu o które został zbudowany filtr wykrywający obserwacje odstające. Szerszy przegląd stylizowanych faktów w finansowych danych wysokiej częstotliwości można znaleźć w pracy \citep{Hautsch2011}.

\subsection{Nierówne odstępy czasowe pomiędzy danymi}

Dane napływają w nieregularnych odstępach czasowych, silnie uzależnionych od aktualnej godziny sesji. Na wykresie \ref{fig:volB} przedstawiono aktywność w początkowym okresie sesji dla instrumentu fZF.H15. W czasie około 4 godzin pojawiło się 3659 obserwacji. Wykres \ref{fig:volB} prezentuje aktywność na tym samym instrumencie w kolejnych 4 godzinach - w tym czasie pojawiło się aż 15975 obserwacji - czyli ponad 4 razy więcej niż w początkowym okresie.

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{wykresy/vol2.PNG}
  \caption{Aktywność zaraz po otwarciu sesji.}
  \label{fig:volB}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{wykresy/vol1.PNG}
  \caption{Aktywność w trakcie trwania sesji.}
  \label{fig:volM}
\end{figure}

\subsection{Różnorodna częstotliwość zdarzeń}

Aktywność na książce zleceń w zależności od instrumentu charakteryzuje się bardzo różną intensywnością. Dla instrumentu przedstawionego na rysunku \ref{fig:cottonOrder} w okresie osiemnastu godzin doszło do 7277 zdarzeń, co daje około jedno zdarzenie na 10 sekund. Natomiast dla kontraktu E-Mini S\&P 500 przedstawionego na rysunku \ref{fig:spticks} w okresie 46 minut doszło do 200000 zdarzeń, co daje około 72 zdarzenia na sekundę. Z przytoczonego przykładu wynika iż aktywność pomiędzy instrumentami może różnić się dramatycznie - obrót na kontraktach terminowych na E-Mini S\&P 500 może być nawet ponad siedemset razy większy niż na kontraktach na bawełnę.


W większości przypadków ilość BidTicków i AskTicków jest zbliżona. Jednak w pewnych sytuacjach mogą wystąpić znaczące różnice (np. w przypadku kontraktu zbliżającego się do wygaśnięcia). Taką sytuację prezentuje rysunek \ref{fig:fplh15} na którym widoczna jest sytuacja iż aktywność po stronie ask zniknęła całkowicie. W takim przypadku należy również rozważyć, czy obszary w których ''znika'' jedna ze stron książki zleceń należy uwzględnić w analizie, gdyż w takich okresach może nie być realnej możliwości handlu.  Na przykład, z uwagi na to, iż kontrakty terminowe mają określony termin zapadalności, charakteryzują się dosyć ściśle określonym schematem aktywności - w pierwszym okresie, zaraz po pojawieniu się danej serii na rynku występuje na nim bardzo mała płynność (a więc aktywność książki zleceń), dopiero w miarę zbliżania się do terminu zapadalności płynność rośnie, by w pewnym momencie załamać się \footnote{Takie załamanie aktywności może właśnie charakteryzować się ''zniknięciem'' jednej ze stron książki zleceń.}.
Efekt załamania się aktywności na danej serii jest widoczny głównie na instrumentach w przypadku których inwestorzy posiadający pozycję w dniu wygaśnięcia kontraktu zobowiązani są do fizycznego dostarczenia towaru. Co więcej, szczegóły dostawy ustalane są w określonym dniu, zwykle na kilka tygodni przed wygaśnięciem kontraktu. W takim przypadku wszyscy inwestorzy niezainteresowani towarem (inwestujący w celach spekulacyjnych) przed tą datą zamykają pozycje i przechodzą na kolejną serię. W przypadku stosowania algorytmicznej strategii inwestycyjnej, należy stworzyć stosowny mechanizm przechodzenia na kolejne serie kontraktów terminowych, tak by nigdy nie wpaść w obszar obniżonej płynności, w którym zamknięcie posiadanej pozycji może być znacząco utrudnione.

\begin{figure}
  \centering
  \includegraphics[scale=0.6]{wykresy/fESH.PNG}
  \caption{Aktywność na marcowym kontrakcie terminowym na E-Mini S\&P 500 w 2015 roku. W ciągu około 46 minut pojawiło się 200000 ticków, co daje około 72 zdarzenia na sekundę.}
  \label{fig:spticks}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.6]{wykresy/fplh15.PNG}
  \caption{Aktywność na marcowym kontrakcie terminowym na pallad w 2015 roku. W pewnym momencie aktywność po stronie ask w zasadzie zniknęła i przez kilkanaście godzin nie pojawiła się żadna zmiana po tej stronie książki zleceń.}
  \label{fig:fplh15}
\end{figure}

\subsection{Występowanie spreadu}

Ze sposobu konstrukcji książki zleceń wynika, iż nie jest możliwe by Bid był większy od Ask, gdyż w takiej sytuacji dochodzi do zawarcia transakcji. Jednakże z faktu, że dane mogą przychodzić asynchronicznie ten warunek nie zawsze musi być spełniony. Rysunek \ref{fig:fpah15} prezentuje taką sytuację, w której $Bid > Ask$, mimo że nie doszło do żadnej transakcji. Należy zaznaczyć, że stworzony na potrzeby pracy program do wizualizacji HFD, w danym punkcie czasowym zaznacza to zdarzenie które miało miejsce pierwsze. Oznacza to, że w przypadku w którym pojawiają się dwa Ticki o tym samym znaczniku czasowym i cenie, kolor na wykresie będzie odpowiadał pierwszemu z nich. Najbardziej wiarygodnym wyjaśnieniem tej sytuacji jest anulowanie zlecenia zaznaczonego w punkcie A, jednak informacja na ten temat dostępna jest dopiero później. Całe zdarzenie miało miejsce w przedziale czasowym mniejszym niż jedna sekunda.


\begin{figure}
  \centering
  \includegraphics[scale=0.6]{wykresy/fpah15.PNG}
  \caption{fESH. Sytuacja w której przez bardzo krótki czas może być obserwowana sytuacja w której $Bid > Ask$, mimo iż w takim przypadku powinna być niemożliwa, gdyż powinno dojść do zawarcia transakcji.}
  \label{fig:fpah15}
\end{figure}

Rysunek \ref{fig:fBRNJ15} przedstawia ewolucję ceny instrumentu fBRNJ15 (górny wykres) wraz z wartością spreadu (dolny wykres). Mimo widocznych zmian Ask i Bid, spread przez większość czasu mieści się w przedziale około $0,018-0,03$, Jedynie w pewnych określonych momentach przekracza tę granicę. Co więcej, spread potrafi zmienić się w bardzo gwałtowny i dynamiczny sposób w bardzo krótkim okresie - rysunek \ref{fig:fBRNJ15Spread} prezentuje zdarzenie w którym w ciągu mniej niż sekundy spread z poziomu ok $0,02$ zwiększył się prawie dwuipółkrotnie do poziomu ok. 0,05, by następnie spaść do poziomu ok. 0,022. 

\begin{figure}
  \centering
  \includegraphics[scale=0.45]{wykresy/fBRNJ15.PNG}
  \caption{Kontrakt terminowy na ropę. Zestawienie ewolucji Bid i Ask z obserwowanym spreadem (dolny wykres).}
  \label{fig:fBRNJ15}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.45]{wykresy/fBRNJ15Spread.PNG}
  \caption{Kontrakt terminowy na ropę. W ciągu mniej niż sekundy spread z poziomu ok $0,02$ zwiększył się prawie dwuipółkrotnie do poziomu ok. 0,05, by następnie spaść do poziomu ok. 0,022.}
  \label{fig:fBRNJ15Spread}
\end{figure}

\chapter{Obserwacje odstające w HFD}

\section{Obserwacje odstające w finansach}

Obserwacje odstające mogą w pewnym stopniu być łączone z różnorodnymi błędami grubymi, dla przykładu obserwacja odstająca może pojawić się na etapie przepisywania danych poprzez przesunięcia przecinka, błędu w odczycie skali miernika itp. Jednakże definiowanie odstawania poprzez występowanie błędów grubych jest niewystarczające, gdyż w pewnych sytuacjach obserwacje mogą być w pełni poprawne, jednak ich użycie może znacząco zaburzyć wykonywane analizy. Przykładem takiej sytuacji mogą być dzienne zwroty dla pary walutowej EURCHF które prezentuje rysunek \ref{fig:eurchf}. Do dnia 14 stycznia 2015 roku dzienne zwroty zawierały się w przedziale od $-0.19\%$, do $0.26\%$. Natomiast od 16 stycznia ten przedział rozszerzył się do $(-1.1 \%; 3\%)$. Obserwacja z dnia 15 stycznia wynosi aż $-18.4\%$. W przypadku prostej, wizualnej analizy obserwacji w celu wykrycia obserwacji odstających taka wartość mogłaby zostać uznana za błąd polegający właśnie na przesunięciu przecinka, jednak jest to wartość wynikła ze zdarzenia które miało miejsce w tym dniu. 15 stycznia 2015 roku Bank Centralny Szwajcarii ogłosił, iż przestanie bronić ustalonego kursu Franka wobec Euro, co spotkało się z bardzo gwałtowną reakcją inwestorów na całym świecie. Jednak mimo tego, iż ta obserwacja nie jest wynikiem błędu, znacząco odbiega od reszty obserwacji, a powtórne wystąpienie wartości na podobnym poziomie jest skrajnie nieprawdopodobne (wynikała ona z kompletnego zaskoczenia uczestników na rynku). Użycie takiej obserwacji w analizach dotyczących zmienności (np. przy estymacji wartości zagrożonej), może prowadzić do znaczącego przeszacowania ryzyka w przyszłych okresach.

Dlatego też na potrzeby pracy obserwacja odstająca będzie definiowana jako obserwacja która w relacji do większości próby budzi zaskoczenie \citep{Ripley2004}. Wadą takiej definicji jest jej brak formalizmu, jednak jest bardzo ogólna, a przez to można ją odnieść do wielu praktycznych zastosowań, bez potrzeby specyfikacji konkretnego modelu odstawania.

Pewnym przykładem wpływu obserwacji odstającej na otrzymane wyniki będzie sytuacja w której strategia przyjmowała przez pierwszy kwartał 2015 roku pozycję długą. Jeżeli za ten okres obliczymy współczynnik Sharpe'a zdefiniowany jako:
\begin{equation}
\label{eq:sharpe}
SH = \frac{\bar{r} - r_f}{sd(r)},
\end{equation}

gdzie $\bar{r}$, to średni zwrot portfela, $r_f$ - stopa wolna od ryzyka (w przykładzie będzie wynosić $0$), natomiast $sd(r)$ to odchylenie standardowe zwrotów. Dla pierwszego kwartału 2015 współczynnik Sharpe'a liczony dla danych dziennych wynosi $-0,09$. Jeżeli natomiast zostanie usunięta obserwacja z 15 stycznia (jedna z 63 obserwacji), współczynnik wyniesie 0,081 - jest to dosyć gwałtowna zmiana. Jednakże decyzja o usunięciu obserwacji odstającej w tym przypadku powinna leżeć w gestii analityka. Z jednej strony - w przypadku franka szwajcarskiego obserwacja odstająca znacząco zaburza wskazania współczynnika Sharpe'a, i pod dyskusję można poddać, czy potencjalnie wygenerowana strata jest wynikiem logiki strategii, która sprawiła, że tego dnia zajęta była pozycja długa, czy też jest to wypadek niezależny od działania strategii i niemożliwy do przewidzenia. W przypadku analizy ryzyka (a więc i potencjalnych strat) sensownym wydaje się mimo wszystko pozostawienie tej obserwacji, gdyż jest ona wyrazem zjawiska wydawać by się mogło skrajnie  nieprawdopodobnego, które jednak miało miejsce. Mimo nieprzewidywalności takich zjawisk muszą one zostać uwzględnione na etapie budowania strategii, gdyż pojedyncza, bardzo duża strata, może zmniejszyć zaufanie inwestorów do konkretnego funduszu inwestycyjnego. W konsekwencji może to doprowadzić do wycofania zainwestowanego kapitału, a tym samym upadku funduszu. Więcej informacji na temat skrajnie nieprawdopodobnych zdarzeń można znaleźć w pracy \citep{Taleb2007}. Natomiast z punktu widzenia jakości sygnału generowanego przez strategię, można założyć, iż strategia nie mogła się obronić przed takim zjawiskiem i mimo, że tego dnia zajęta pozycja przyniosła katastrofalną stratę, w długim okresie generowany sygnał co do zajętej pozycji może być sensowny.



\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{wykresy/EURCHF}
  \caption{Dzienne zwroty na parze walutowej EURCHF. Obserwacja z dnia 2015-01-15 wyraźnie odbiega od reszty wartości, jednak jest obserwacją poprawną, która wynika z decyzji Centralnego Banku Szwajcarii ogłoszonej tego dnia.}
  \label{fig:eurchf}
\end{figure}


\section{Zastosowanie metod statystyki odpornej}

Każda procedura statystyczna konstruowana jest przy założeniu spełnienia określonych założeń, co do mechanizmu generującego dane. Przykładowo, dane generowane są przez rozkład normalny, a obserwacje są niezależne. Niestety w praktycznych przypadkach bardzo często nie można zapewnić iż wymagane przez procedurę założenia są spełnione. W takich przypadkach dalsze wnioskowanie statystyczne może być nieuprawnione, gdyż albo nieznane są w ogóle własności tych procedur, albo wymagania co do jakości uzyskanych estymatorów (obciążenie, efektywność, etc.) są niemożliwe do spełnienia.

Podejście proponowane przez statystykę odporną ma na celu prezentację procedur dających wiarygodne oszacowania nie tylko w przypadku, gdy dane generowane są przez zakładany rozkład, lecz również w przypadku gdy rozkład generujący dane odbiega od zakładanego. Odporna procedura statystyczna powinna dawać wiarygodne oszacowania również w przypadku gdy analizowany zbiór danych zawiera obserwacje odstające \citep{Van:2000}. Przykłady użycia odpornej procedury statystycznej prezentują rysunki \ref{lad1} i \ref{lad2}. Szersze omówienie zagadnień związanych z zastosowaniem metod statystki odpornej można znaleźć w pracach \citep{dkzz1}i \citep{DK2012}.

\begin{figure}[H]
\begin{subfigure}[t]{0.45\textwidth}
  \includegraphics[width=\linewidth]{wykresy/ladout}
	\caption{Obserwacja odstająca ze względu na zmienną objaśniającą.}
	\label{lad1}  
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
  \includegraphics[width=\linewidth]{wykresy/ladouty}
  \caption{Obserwacja odstająca ze względu na zmienną objaśnianą.}
  \label{lad2}
\end{subfigure}
\caption{Przykładowe porównanie działania klasycznych (MNK i LAD) i odpornych (NGR - metoda największej głębi regresyjnej) estymatorów prostej regresji.}
\caption*{Źródło: Obliczenia własne - R Project}
\label{fig:lad}
\end{figure}

W przypadku prezentowanego problemu występowania obserwacji odstających w danych finansowych zastosowanie procedur statystyki odpornej wydaje się być uzasadnione, a nawet konieczne, jednak napotyka na pewne praktyczne problemy.

Procedury odporne są bardziej złożone obliczeniowo od swoich klasycznych odpowiedników. O ile w przypadku potrzeby jednorazowej estymacji modelu z reguły różnice w czasie wykonywania nie są bardzo istotne, to specyfika testów jakim poddawane są strategie algorytmiczne wymaga wielokrotnej estymacji modelu, w której te różnice zyskują na znaczeniu, często nie pozwalając w ogóle na przeprowadzenie wiarygodnych testów w sensownym czasie. Dla przykładu, do wyznaczania wag dla portfela składającego się z 30 aktywów wykorzystywany jest klasyczny model Markowitza. Macierz kowariancji używana w obliczeniach wyznaczana jest na dwa sposoby, pierwszy wykorzystuje klasyczny estymator macierzy kowariancji, natomiast w drugim przypadku używany jest estymator MCD. W obu przypadkach brane jest pod uwagę ostatnie 504 obserwacje (dwa lata). Pojedynczy czas obliczeń w przypadku klasycznym wynosi około 0.0003s, natomiast dla MCD około 1 sekundy. W symulacji na danych zawierających 10 lat, w której macierz kowariancji pomiędzy aktywami musi być obliczana dla każdego dnia, czas potrzebny na obliczenia estymatora klasycznego wyniesie łącznie ok 0.75 sekundy, natomiast dla estymatora MCD będą to aż 42 minuty. Zastosowanie estymatora odpornego tylko w tym jednym elemencie strategii sprawi, że czas obliczeń wydłuży się w znaczącym stopniu, przez co w danym okresie czasu będzie możliwe przetestowanie mniejszej ilości wariantów strategii. Równocześnie jedną z istotnych kwestii jest zbadanie stabilności otrzymywanych wyników w pewnym otoczeniu parametrów strategii. W omawianym przykładzie mogłoby to wiązać się, ze sprawdzeniem jak zmieniają się wyniki w zależności od ilości obserwacji używanych do estymacji macierzy kowariancji. W takim przypadku, przy próbie sprawdzenie 50 różnych wariantów długości okna, obliczenia dla MCD zajęłyby około 50 godzin, przy czym jakiekolwiek inne obliczenia związane z logiką strategii są pomijane. 

Drugi aspekt związany jest z samą specyfiką testów strategii inwestycyjnych. W klasycznie prowadzonej analizie statystycznej po estymacji modelu przeprowadza się jego weryfikację. W tym celu wykorzystuje się testy dopasowania, zdolności prognostyczne, bądź klasyfikacyjne na odpowiednio przygotowanych zbiorach testowych. W przypadku strategii algorytmicznych, stosowane procedury statystyczne wykorzystywane są do generowania sygnałów określających pozycję jaką w danej chwili czasowej powinna zająć strategia. Sama pozycja może być utrzymywana przez określony czas symulacji (np. do zakończenia sesji w danym dniu symulacji), bądź do otrzymania sygnału zamknięcia, lub sygnału otwarcia pozycji przeciwnej. Następnie na podstawie szeregu cen dla instrumentu wyznaczana jest wartość pozycji i kolejne jej zmiany, aż do zamknięcia. Następnie na podstawie pozycji które zajmowała strategia generowane są krzywe kapitału strategii. Stosowanie procedur odpornych do wyznaczania sygnałów, po to by uchronić się przed obserwacjami odstającymi w danych w pewnym sensie legitymuje ich występowanie w szeregu. W takim przypadku mogą one mieć wpływ na wyznaczone wartości pozycji, a za tym idzie na wiarygodność wyniku prezentowanego przez strategię, co w dalszym kroku nakładałoby wymóg stosowania również odpornych metod do weryfikacji rezultatów strategii, uniemożliwiając stosowanie metod klasycznych , takich jak wskaźnik Sharpe.  

Dlatego też bardziej odpowiednim wydaje się usuwanie obserwacji odstających we wczesnym etapie symulacji, tak by nie miały one wpływu na stosowane procedury statystyczne, jak również wyznaczane wartości pozycji. Jednocześnie wykorzystanie odpornych procedur statystycznych mających chronić przed odstępstwami od zakładanych rozkładów w dalszym ciągu jest uzasadnione.

\section{Przykłady obserwacji odstających w HFD}

Obserwacje odstające w danych wysokiej częstotliwości przejawiają się na wiele sposobów, przez co trudno zaproponować jeden, ogólny model ich powstawania. Rysunek \ref{fig:singleCT} prezentuje sytuację, w której pojawia się pojedyncza obserwacja odstająca, daleko od głównej zbiorowości danych. Takie sytuacje, mimo tego iż w występują w danych dosyć często, są łatwe do wychwycenia. Dużo trudniejsze w czyszczeniu są dane przedstawione na wykresie \ref{fig:f6LJ15}, w których oprócz pojedynczych obserwacji odstających występują całe serie.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{wykresy/singleCT.PNG}
  \caption{Pojedyncza obserwacja pojawiająca się w danych wysokiej częstotliwości.}
  \label{fig:singleCT}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{wykresy/f6LJ15.PNG}
  \caption{Zanieczyszczony szereg obserwacji wysokiej częstotliwości. Oprócz pojedynczych obserwacji odstających, pojawiają się ich całe serie.}
  \label{fig:f6LJ15}
\end{figure}

\subsection{Poziome serie obserwacji obserwacji odstających}

W pewnych przypadkach pojawiają się całe serie obserwacji odstających, na tym samym poziomie ceny. Rysunek \ref{fig:f6mh15} prezentuje sytuację w której obserwacje odstające pojawiają się wśród ticków transakcyjnych. Przedstawiona sytuacja jest bardzo zaskakująca z punktu widzenia książki zleceń, gdyż transakcje powinny zachodzić w obrębie cen bid i ask, a jedynie w specyficznych przypadkach mogą od nich nieznacznie odbiegać. Jednakże w przedstawionym przykładzie TransTicki dale odbiegają od poziomu cen Bid i Ask.

Równie ciekawa sytuacja zaprezentowana jest na rysunku \ref{fig:f6LJ15Row}, na którym oprócz standardowej trajektorii cen Bid i Ask, pojawiają się ustawione w poziomej linii obserwacje, które w świetle zachowania reszty danych wydają się być odstające. Jedną z hipotez wyjaśniających powstawanie tego typu zjawisk może być błędy w przesyłaniu niższych poziomów książki zleceń, przez co oznaczane są one jako obserwacje z poziomu pierwszego. Jednak weryfikacja tejże hipotezy wymagałaby przeprowadzenia audytu systemu nadawcy jak i odbiorcy danych, co dalece wykracza poza ramy niniejszej pracy.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{wykresy/f6mh15.PNG}
  \caption{Seria obserwacji odstających wśród ticków transakcyjnych. Należy zwrócić uwagę iż wszystkie obserwacje pojawiają się na tym samym poziomie.}
  \label{fig:f6mh15}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{wykresy/f6LJ15Row.PNG}
  \caption{Serie poziomych (mających tę samą wartość) obserwacji odstających ze względu na Bid i Ask.}
  \label{fig:f6LJ15Row}
\end{figure}

\subsection{Pionowe trajektorie obserwacji odstających}

Na rysunku \ref{fig:fCNH15} przedstawiono serię obserwacji odstających idącą prawie górę, zmieniającą cenę ask z poziomu w okolicach 500, do wartości w granicach 900 w okresie mniejszym niż jedna sekunda (por. Rys. \ref{fig:fCNH15ZOOM}. Tego typu zmiana, zdaniem autora nie może być sensownie wytłumaczona i wszystkie te obserwacje należy uznać za odstające.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{wykresy/fCNH15.PNG}
  \caption{Idąca pionowo w górę seria obserwacji odstających, powodująca gwałtowną zmianę ceny po jednej ze stron książki zleceń w bardzo krótkim czasie.}
  \label{fig:fCNH15}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{wykresy/fCNH15ZOOM.PNG}
  \caption{Zbliżenie na sytuację przedstawioną na Rys. \ref{fig:fCNH15}. Seria obserwacji odstających powodująca gwałtowną zmianę ceny pojawiła się w czasie mniej niż jednej sekundy.}
  \label{fig:fCNH15ZOOM}
\end{figure}


Jednocześnie w pewnych bardzo rzadkich okolicznościach podobne zdarzenia mogą faktycznie mieć miejsce i nie mogą zostać uznane za błędy systemu przesyłającego dane, a tym samym obserwacje odstające. Przykładami takich zdarzeń mogą być ''flash crash'', czyli gwałtowne zmiany ceny instrumentu rynkowego powstałe między innymi pod wpływem interakcji strategii wykorzystujących handel wysokiej częstotliwości \citep{Easley2010}.

\chapter{Prezentacja filtra dla finansowych danych wysokiej częstotliwości}

\newcommand{\Spt}{\ensuremath{Sp_{t}} }

\newcommand{\MSpc}{\ensuremath{Sp_{t|t-1}} }
\newcommand{\MSpn}{\ensuremath{Sp_{t+1|t}} }
\newcommand{\MSpo}{\ensuremath{Sp_{t-1|t-1}} }

%% Czas
\newcommand{\ts}{\ensuremath{{t}} }
\newcommand{\tsl}{\ensuremath{{t-1}} }


%% Parametry filtra


Algorytmy filtracji danych finansowych zwykle objęte są tajemnicą handlową i nie są upubliczniane, z tego też powodu literatura w tym temacie jest bardzo uboga. Najpełniejszy opis związany z usuwaniem obserwacji odstających można znaleźć w pracy \citep{muller2010filtering}, będącej opisem patentu na system wykrywający obserwacje odstające. Z uwagi, iż jest to system płatny, znacznie bardziej złożony i wymagający (np. wymagane jest wykorzystywanie więcej niż jednego źródła danych, w celu wzajemnego potwierdzania przychodzących obserwacji pomiędzy tymi źródłami), nie istniała możliwość wykorzystania tego systemu do analizy danych przeznaczonych na potrzeby niniejszej pracy. Dlatego też stworzono projekt autorskiego filtra, którego celem jest uzyskanie jak najlepszych rezultatów w usuwaniu obserwacji odstających na posiadanych danych.

\section{Główne założenia}

Prezentowany filtr ma za zadanie usuwać obserwacje odstające, opierając się na stylizowanych faktach dotyczących danych wysokiej częstotliwości, jak również zaobserwowanych charakterystykach pojawiających się obserwacji odstających. Jednocześnie stara się wziąć pod uwagę cztery praktyczne wymagania pozwalające na jego implementację w rzeczywistym środowisku produkcyjnym:


\begin{itemize}
\item Dane mają być przetwarzane on-line. Oznacza to, że filtr musi działać bardzo szybko - tak by analiza pojedynczej obserwacji trwała krócej niż czas do pojawienia się kolejnej. Jednocześnie nie ma możliwości powrotu do obserwacji już przeanalizowanej (obserwacje nie są przechowywane).
\item Filtr powinien posiadać zdolność samo-naprawy w przypadku nieprzewidzianych zdarzeń. Niedopuszczalne jest ''zakleszczenia się'' filtra, jest to sytuacja w której wszystkie kolejne obserwacje z jednej, lub drugiej strony książki zleceń są odrzucane.
\item Koszt odrzucenia poprawnej obserwacji jest znacznie niższy, niż koszt przyjęcia za poprawną obserwacji odstającej. 
\item Filtr jest w pełni deterministyczny.
\end{itemize} 

Pierwsze założenie wynika z wymagań obliczeniowych. Filtr jest jednym z elementów który przez pewien moment przetrzymuje obserwację, zanim trafi ona do dalszej części silnika zarządzającego strategią inwestycyjną. Oznacza to, że zastosowanie filtra wprowadza pewne opóźnienie, które sprawia, że strategia otrzymuje dane już w pewien sposób zdezaktualizowane. Wiąże się to z możliwością  zbyt spóźnionej reakcji na pewne gwałtowne zjawiska rynkowe. Jednocześnie czas na analizę musi być mniejszy niż okres pomiędzy obserwacjami, inaczej opóźnienie przesyłania danych z filtra będzie się nawarstwiać.

Należy również zwrócić uwagę, iż filtr będzie  stosowany równocześnie dla bardzo dużej liczby aktywów finansowych, przez co złożoność obliczeniowa jest dodatkowo zwiększana i może znacząco obciążać architekturę sprzętową. Z tego też powodu wprowadzono założenie, iż po jednokrotnym sprawdzeniu obserwacji nie ma już do niej powrotu. Poprzez takie działanie znacząco minimalizowana jest złożoność pamięciowa realizowanego algorytmu.

Drugie założenie odnosi się do bardzo złożonych zjawisk występujących na rynku które mogą doprowadzić do sytuacji w której filtr w pewien sposób się ''zakleszczy'', co znaczy, że wszystkie kolejne obserwacje będą odrzucane. Przykłady takich zdarzeń rynkowych  pojawią się wraz z omówieniem wprowadzonych mechanizmów samo-naprawy w filtrze prezentowanym w niniejszej pracy.

Przypadek zakleszczenia się filtra jest bardzo groźny z punktu widzenia strategii, gdyż oznacza, że dane nie są przepuszczane do strategii, przez co niejako zostaje zatrzymane zarządzanie pozycją. Rozwiązaniem tego problemu jest stworzenie stosownego systemu ostrzeżeń, informującego o pojawiających się problemach (np. długa seria odrzuconych obserwacji). Jednak taki system wymaga nieustannego monitorowania (handel na międzynarodowym rynku odbywa się dwadzieścia cztery godziny na dobę), przez co może być niepraktyczny w realizacji. Dlatego też bardziej sensownym rozwiązaniem wydaje się wbudowanie pewnych metod naprawczych w sam filtr.

Kolejne założenie ma na celu uwzględnienie natury wysokiej częstotliwości danych finansowych, w których czas pomiędzy kolejnymi obserwacjami jest bardzo krótki. Dlatego też nawet  uznanie poprawnej obserwacji za odstającą nie jest zbyt znaczące, gdyż na jej miejsce szybko pojawi się następna, jednocześnie przepuszczenie obserwacji odstającej ma szerokie konsekwencje w zasadzie w każdym dalszym module strategii.

Założenie określające, iż filtr jest w pełni deterministyczny jest pewnym założeniem praktycznym, umożliwiającym uzyskanie powtarzalnych wyników w kolejnych iteracjach backtestów. Oczywiście w przypadku modeli wykorzystujących optymalizację nieliniową opierającą się na liczbach pseudolosowych (np. metody MCMC) spełnienie takiego założenia można zapewnić poprzez ustalenie ziarna dla generatora tychże liczb. Jednakże tego typu metody zwykle są złożone obliczeniowo, a przez to niepraktyczne w tego typu zastosowaniach.  



\subsection{Prezentacja algorytmu}

W algorytmie będą używane następujące oznaczenia:

\begin{itemize}
\item \ts - obecna chwila czasowa.
\item \tsl - czas ostatniego ticku.
\item \Spt  - spread w chwili \ts
\item \MSpc - średni spread w chwili \ts, obliczony na podstawie obserwacji do chwili \tsl.
\item \MSpn - średni spread w chwili \ts, po aktualizacji tickiem z tej chwili czasowej.
\item \MSpo - średni spread w chwili \tsl, po aktualizacji.
\end{itemize}

Jednocześnie filtr posiada pewne stałe parametry które muszą zostać ustawione:

\begin{itemize}
\item $minSpread$ - minimalny spread dla instrumentu.
\item $SprMulti$ - mnożnik średniego spreadu.
\item $\phi_{min}$ - minimalny udział nowej obserwacji w średnim spreadzie.
\item $\phi_{max}$ - maksymalny udział nowej obserwacji w średnim spreadzie.
\item $rmVal$ - wartość korygująca średni spread po odrzuceniu obserwacji.
\end{itemize}

W listingu \ref{FiltrBID} przedstawiono algorytm filtracji pojedynczej ceny Bid. Algorytm filtracji ceny Ask jest analogiczny, jedyną różnicą są zamienione znaki $leq$ i $req$, dlatego też nie będzie omawiany.



\IncMargin{5em}
\begin{algorithm}
\KwData{\\
BidPrice - nowa cena Bid.\\
time - czas wystąpienia tiku.}
\KwResult{Wartość logiczna - false w przypadku odrzucenia tiku}

\Spt = AskPrice - BidPrice; \tcc*[r]{AskPrice to ostatnia, nie odrzucona cena Ask.}



\nlset{Warunek 1}\label{War1}\If{$\Spt \leq SprMulti \cdot \MSpc$ lub \\ 
\nlset{Warunek 2}\label{War2}$BidPrice \leq AskPrice + CrossRange$ }
{
\nlset{Zabezpieczenie 1}\label{Zab1}	\Spt = lastDelAsk - BidPrice; \tcc*[r]{lastDelAsk to ostatnia, nie odrzucona cena Ask.}

	\If{$\Spt \leq SprMulti \cdot \MSpc$ lub \\ 
	$BidPrice \leq lastDelAsk + CrossRange$}
	{
		lastDelBid = BidPrice\\
\nlset{Zabezpieczenie 2}\label{Zab2} AskPrice = AskPrice + RmVal\\


	\Return{False}
	
	}
	
\nlset{Uwaga 1}\label{Uwaga1}	AskPrice = lastDelBid
	
}
\Return{True}

\caption{Algorytm filtracji cany Bid\label{FiltrBID}}
\end{algorithm}
\DecMargin{5em}

\ref{War1} testuje, czy nowy spread nie odbiega w znaczący sposób od średniego spreadu, co sugeruje obserwacje odstającą. 

Natomiast \ref{War2} ma na celu wychwycenie sytuacji, w której Bid przewyższa Ask w znaczący sposób. Samo zastosowanie warunku by $Bid < Ask$ jest niewystarczające, gdyż dane pojawiają się w sposób asynchroniczny, przez co może dojść do sytuacji przedstawionej na rysunku \ref{fig:Cross}. Dodatkowa granica \emph{CrossRange} pozwala wyłapać takie zdarzenia, znacząco zmniejszając ilość niepotrzebnie odrzucanych ticków.

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm, height=80mm]{wykresy/cross}
  \caption{Możliwa sytuacja w przypadku zastosowania prostego warunku odrzucającego ceny Bid, gdy $Bid > Ask$. W przypadku szybkiego wzrostu ceny (łączącego się ze wzrostem po obu stornach książki zleceń), obserwacje Bid mogą być zawsze odrzucane.}
  \label{fig:Cross}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm, height=80mm]{wykresy/relacjaBIDASK}
  \caption{Możliwa do zaistnienia sytuacja w przypadku skoku ceny i braku pierwszego zabezpieczenia wprowadzonego w filtrze, powodującego sprawdzenie ''odstawania'' również względem ostatniej odrzuconej obserwacji.}
  \label{fig:JUMP}
\end{figure}


W linijce \ref{Zab1} spread przeliczany jest powtórnie, jednak z wykorzystaniem ostatniej odrzuconej ceny Ask. Powtórne sprawdzenie poprawności ticku pozwala zabezpieczyć działanie filtra w przypadkach dużych skoków cen. Scenariusz sytuacji występującej bez zastosowania takiego zabezpieczenia przedstawia rysunek \ref{fig:JUMP}. Jednocześnie może to prowadzić do zaakceptowania jako poprawnej sytuacji, w której krótko po sobie nastąpiły dwie obserwacje odstające po przeciwnej stronie książki zleceń (rysunek \ref{fig:NEXTOUT}).

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm, height=80mm]{wykresy/nextout}
  \caption{Sytuacja która może spowodować przyjęcie obserwacji odstającej w oparciu o pierwsze zabezpieczenie. Pierwsza obserwacja Ask zostanie odrzucona, jednak pojawiający się po niej Bid, jeżeli zostanie zweryfikowany względem niej, może zostać przyjęty (spread względem odrzuconej ceny Ask nie odbiega od spreadu pomiędzy wcześniejszymi obserwacjami).}
  \label{fig:NEXTOUT}
\end{figure}

Linia \ref{Zab2} jest kolejnym zabezpieczeniem przeciwko możliwości zakleszczenia się filtra. W przypadku gdy dana cena Bid jest odrzucana, poprzez wartość RmVal następuje  podniesienie ceny Ask używanej do obliczeń wewnątrz filtra (sam filtr nie modyfikuje wartości tików). Powoduje to, iż nawet w przypadku dużej asymetrii w ilości pojawiających się obserwacji po jednej stronie książki zleceń przekraczających granicę CrossRange, nowe obserwacje w końcu zostaną zaakceptowane (por. rys. \ref{fig:longStreak}).

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm, height=80mm]{wykresy/longStreak}
  \caption{Sytuacja obrazująca motywację wprowadzenia drugiego zabezpieczenia związanego z podnoszeniem ceny Ask po odrzuceniu obserwacji Bid. Dzięki temu w takiej sytuacji tylko część początkowych Bidów zostanie odrzucona. W przeciwnym przypadku odrzucone zostałyby wszystkie obserwacje, aż do pojawienia się końcowej ceny Ask.}
  \label{fig:longStreak}
\end{figure}

%\begin{figure}[H]
%  \centering
%  \includegraphics[width=0.6\textwidth]{wykresy/relacjaBIDASK.pdf}
%  \caption{Przypadek zakleszczenia się filtra spowodowany nagłym skokiem ceny.}
%  \label{fig:JUMP}
%\end{figure}

\subsection{Aktualizacja \MSpn}

Listing \ref{filtrapdate} przedstawia sposób aktualizacji średniego spreadu \MSpn:
\IncMargin{5em}
\begin{algorithm}

\eIf{TickAccepted}
{ $\Delta t = t - lastTime$\\
\nlset{Uwaga 1}\label{uwg1ta}	$\phi = exp(\Delta t / \lambda)$\\
\nlset{Uwaga 2}\label{uwg2ta}	$\phi = max(\phi, \phi_{min})$\\
\nlset{Uwaga 3}\label{uwg3ta} 	$\phi = min(\phi, \phi_{max})$ \\ 
\nlset{Aktualizacja}\label{line:akt}	$\MSpn = \phi \cdot \MSpc + (1 - \phi) \Spt$
}
{
\nlset{Zabezpieczenie 3}\label{Zab3} \MSpn = \MSpc + RmVal\\
}
\DecMargin{5em}

\caption{Algorytm aktualizacji \MSpn \label{filtrapdate}}
\end{algorithm}

Nowa wartość \MSpn jest ważoną średnią poprzedniej wartości i aktualnego spreadu. Przy czym  waga \Spt zależna jest od czasu pomiędzy dwiema ostatnimi obserwacjami (\ref{uwg1ta}). Im krótszy czas, tym wpływ nowej wartości spreadu jest większy. Dostosowywanie wartości $\phi$ odbywa się poprzez parametr $\lambda$ - większe wartości $\lambda$ oznaczają większy wpływ \Spt na wartość \MSpn. 

Dodatkowo wprowadzono parametr $\phi_{max}$, określający maksymalny udział aktualnego spreadu (\Spt) w \MSpn. Ma on na celu zapobieganie sytuacjom zbytniego zmniejszenia się średniego spreadu, gdy po dłuższym okresie jako pierwszy pojawi się tick którego spread względem starej, przeciwnej ceny będzie bardzo mały, a spread dla następnych tików się powiększy (lub utrzyma się na podobnym poziomie). W takim wypadku kolejne obserwacje będą odrzucane, aż do momentu odpowiedniego rozszerzenia się spreadu związanego z \ref{Zab3}. W przedstawionej sytuacji udział \Spt zostanie ograniczony do wartości $\phi_{max}$. Przykładowe zdarzenie tego typu pokazane jest na rys. \ref{fig:longTime}. Jednocześnie należy zauważyć, że w pewnych przypadkach może to prowadzić do sytuacji, w której spread będzie znacząco zawyżony. 

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm, height=80mm]{wykresy/longTime}
  \caption{Sytuacja motywująca wprowadzenie maksymalnego udziału nowej obserwacji w średnim spreadzie. W filtrze po długiem okresie, większa wartość jest przypisywana nowej obserwacji, jednak gdyby była zbyt duża, mogłoby dojść do sytuacji przedstawionej na rysunku w której względem starej ceny Ask, spread nowego Bida jest bardzo mały, przez co znacząco zaniżyłby wskazanie średniego spreadu, powodując odrzucenie kolejnych obserwacji Ask (których spread jest podobnej wartości jak przed przerwą w danych).}
  \label{fig:longTime}
\end{figure}

Jednocześnie w sytuacji dużych odstępów między danymi ujawnia się również dodatkowa słabość filtra. Dla pierwszej obserwacji po długim okresie spread może być liczony względem ceny która jest już nieaktualna, jednak nowa cena jeszcze się nie pojawiła. W obecnym podejściu do filtracji danych, w którym założone jest działanie online (po sprawdzeniu ticku nie ma już do niego powrotu), obejście tego problemu jest bardzo trudne (o ile w ogóle możliwe). W pewnym stopniu rozwiązaniem jest wprowadzone wcześniej zabezpieczenie \ref{Zab1}, które nie dopuści do zakleszczenia się filtra po skoku ceny.

Parametr $\phi_{min}$ określa minimalny udział \Spt w \MSpn. Przyspiesza on dostosowywanie się wartości średniego spreadu dla płynnego rynku, w którym czas pomiędzy kolejnymi obserwacjami jest bardzo krótki.

Linijka \ref{Zab3} jest kolejnym zabezpieczeniem przeciwko zakleszczeniu się filtra. Rozszerzenie spreadu o wartość $RmVal$ pozwala na samo-naprawienie się filtra w sytuacji w której nastąpił gwałtowny skok spreadu, kosztem kilku pierwszych obserwacji (por. \ref{fig:spreadJump}). Takie zabezpieczenie posiada oczywistą wadę w przypadku całej serii złych danych - powolne rozszerzanie się średniego spreadu w końcu spowoduje, że obserwacje odstające zostaną uznane za poprawne (por \ref{fig:outSeries}). W tym przypadku należy dokonać wyboru dotyczącego wrażliwości filtra na długie serie złych danych, a szybkością samonaprawy w przypadku zakleszczenia. 

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm, height=80mm]{wykresy/spreadJump}
  \caption{Sytuacja motywująca wprowadzenie trzeciego zabezpieczenia rozszerzającego średni spread po każdej odrzuconej obserwacji. Dzięki temu w przypadku gwałtownego skoku spreadu, po kilkunastu obserwacjach średni spread rozszerzy się na tyle, by zacząć przyjmować nowe obserwacje. W przeciwnym przypadku wszystkie obserwacje z tak powiększonym spreadem zostałby odrzucone, przez co do strategii nie docierałby żadne dane.}
  \label{fig:spreadJump}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm, height=80mm]{wykresy/outSeries}
  \caption{Sytuacja powodująca przyjęcie obserwacji odstających spowodowana trzecim zabezpieczeniem. W przypadku dużej serii obserwacji odstających stopniowe rozszerzanie średniego spreadu sprawi, iż w końcu zostaną one uznane za poprawne.}
  \label{fig:outSeries}
\end{figure}


\chapter{Ocena działania filtra}

\section{Metodologia}

Kluczową sprawą w testowaniu działania filtra danych jest możliwość stworzenia stosownego środowiska testowego, które w wiarygodny sposób symuluje rzeczywiste zachowanie się książki zleceń. W takim środowisku obserwacje odstające znane są apriori (tzn. wiadomo którą obserwację proces symulujący książkę zleceń wygenerował jako odstającą), wtedy ocena jakości filtra może być przeprowadzona w oparciu o ilość odrzuconych obserwacji, które w rzeczywistości nie były odstające, jak również ilość zaakceptowanych obserwacji odstających.

Sam model książki zleceń, umożliwiający symulacje jest bardzo złożoną kwestią. W literaturze prezentowane są między innymi modele oparte na aukcjach \citep{doubleAuction}, łańcuchach Markowa \citep{huang2015simulating}, jak również wykorzystujące systemy agentowe \citep{chiarella2009impact}. Jednakże prezentowane modele do estymacji parametrów wymagają informacji na temat ewolucji całej książki zleceń na wszystkich poziomach. Wykorzystane w niniejszej pracy dane dotyczą jedynie ewolucji pierwszego poziomu książki zleceń, przez co są dalece niepełne by móc je wykorzystać w którymkolwiek z prezentowanych modeli. Jednocześnie klasa modeli \emph{Autoregressive Conditional Duration} (ACD) zaproponowanych w \citep{engle1998} z uwagi na występujące w danych zanieczyszczenie wymagałaby stworzenia stosownych, odpornych procedur estymacji co dalece wykracza poza ramy niniejszej pracy, dlatego też przyjęto inną, dużo mniej doskonałą metodę oceny jakości filtra danych. W pierwszej fazie przeprowadzono subiektywne, ręczne czyszczenie rzeczywistych danych. Następnie wyniki czyszczenia ręcznego zostały porównane z czyszczeniem z wykorzystaniem filtra. 

\section{Wyniki filtrowania}

\begin{table}
\centering

\begin{tabular}{|c|c|}
\hline
Parametr & Wartość \\
\hline
$minSpread$ & $2 \cdot TickSize$ \\
$\phi_{min}$ & 80\% \\
$\phi_{max}$ & 95\% \\
$rmVal$ & $0.25 \cdot TickSize$\\
\hline
\end{tabular}
\caption{Parametry filtra wykorzystane w symulacjach.}
\label{params2}
\end{table}

Do oceny jakości filtrowania wykorzystano dwie miary - \emph{True Positive  Rate} (TPR) i \emph{False Positive Rate}(FPR). TPR zdefiniowano jest jako:
\begin{equation}
TPR = \frac{TP}{TP + FN},
\end{equation}
gdzie $TP$ określa ilość obserwacji poprawnie sklasyfikowanych jako odstające, a $FN$ liczbę obserwacji odstających które zostały sklasyfikowane jako poprawne. Wartość TPR przedstawia jaki procent wszystkich obserwacji odstających został wykryty. Natomiast FPR zdefiniowano jako:
\begin{equation}
TPR = \frac{FP}{FP + TN},
\end{equation}
gdzie $FP$ oznacza liczbę obserwacji uznanych za odstające, mimo że nimi nie były. Natomiast $TN$ określa ilość obserwacji, które zarówno przez filtr jak i osobę filtrującą dane zostały uznane za poprawne. Wyniki dla trzech różnych wartości mnożnika spreadu przedstawiono w tabeli \ref{tab:result1}. Zawiera ona średnie wartości TPR i FPR dla kolejnych serii kontraktów na poszczególny instrument finansowy. Pozostałe parametry użyte w procesie filtracji zaprezentowano w tabeli \ref{params2}. Należy zwrócić uwagę, iż modyfikacja tych parametrów nie ma aż takiego wpływu na działanie filtra jak zmiana mnożnika $SprMulti$.


Filtr generalnie dobrze radzi sobie w sytuacjach, w których obserwacje odstające pojawiają się pojedynczo (por. rys. \ref{fig:ojfh10}), natomiast w zupełności nie radzi sobie w przypadku poziomych i pionowych ścieżek obserwacji odstających (por. rys. \ref{fig:fzwh11}). Co więcej występowanie ścieżek obserwacji odstających w dużym stopniu zaburza wskazania TPR. Założenia na których opiera się filtr wymagają by był on samonaprawialny, to znaczy, by nie było możliwości, że jakiekolwiek zdarzenie na rynku (np. gwałtowny skok spreadu) spowoduje, że wszystkie kolejne obserwacje zostaną odrzucone. By wymusić takie zachowanie, zastosowano zabezpieczenie rozszerzające spread w sytuacji odrzucenia obserwacji. W przypadku pojedynczej obserwacji odstającej nie jest to problemem, gdyż chwilowe rozszerzenie spreadu zostanie praktycznie natychmiast zmniejszone wraz z kolejną, poprawną obserwacją. Jednak w wypadku serii obserwacji odstających powoduje to, iż część obserwacji z takiej serii może (po odpowiednio dużym rozszerzeniu spreadu)zostać zaakceptowana jako poprawna. Co więcej, jeżeli rozszerzanie się spreadu, powodujące w konsekwencji pojawienie się obserwacji odstających odbywa się w dosyć płynny sposób (odległości pomiędzy kolejnymi tickami nie są duże), wtedy żadna z obserwacji nie zostanie uznana za odstającą (do takiej sytuacji doszło na rysunku \ref{fig:fzwh11}). Jednocześnie należy zwrócić uwagę, że serie obserwacji odstających mogą składać się nawet z kilkuset punktów, więc jeżeli dla danego instrumentu typowym sposobem realizacji odstawania są serie obserwacji odstających, to TPR dla proponowanego filtra będzie bardzo niski. 

\begin{longtable}{llll}
  \hline
Symbol & Mnożnik 2 & Mnożnik 2.5 & Mnożnik 3 \\ 
  \hline
 OrangeJuice & 82.39\%/2.63\% & 79.65\%/0.92\% & 76.95\%/0.34\% \\ 
  Silver & 79.89\%/0.18\% & 76.96\%/0.09\% & 73.2\%/0.07\% \\ 
  Palladium & 68.29\%/2.3\% & 63.31\%/1.28\% & 62.03\%/0.95\% \\ 
  SwedishKrone & 66.34\%/0.61\% & 61.47\%/0.38\% & 56.19\%/0.31\% \\ 
  Rand & 64.39\%/1.64\% & 51.57\%/0.99\% & 44.06\%/0.77\% \\ 
  Coffee & 62.68\%/0.47\% & 56.69\%/0.24\% & 50.73\%/0.15\% \\ 
  Ruble & 61.66\%/0.65\% & 53.65\%/0.36\% & 47.86\%/0.18\% \\ 
  miniDJIA & 61.56\%/0.02\% & 57.2\%/0.01\% & 52.23\%/0\% \\ 
  Brent & 59.71\%/0.33\% & 52.99\%/0.18\% & 47.73\%/0.11\% \\ 
  Copper & 58.54\%/4.76\% & 49.91\%/4.01\% & 43.8\%/3.81\% \\ 
  EURJPY & 57.53\%/0.05\% & 46.79\%/0.02\% & 37.66\%/0.01\% \\ 
  NorwegianKrone & 57.46\%/0.68\% & 49.48\%/0.59\% & 43.28\%/0.52\% \\ 
  LongGilt & 56.2\%/0.1\% & 53.08\%/0.1\% & 50.62\%/0.07\% \\ 
  Milk & 55.93\%/2.75\% & 47.35\%/0.78\% & 37.52\%/0.24\% \\ 
  EuroStoxx & 55.89\%/0.11\% & 53.56\%/0.11\% & 53.09\%/0.11\% \\ 
  EURGBP & 54.38\%/0.24\% & 44.42\%/0.09\% & 36.88\%/0.06\% \\ 
  Cocoa & 53.98\%/0.42\% & 49.5\%/0.28\% & 45.7\%/0.25\% \\ 
  miniGold & 53.16\%/0.51\% & 46.56\%/0.38\% & 43.18\%/0.35\% \\ 
  Real & 52.5\%/0.4\% & 40.36\%/0.16\% & 30.33\%/0.13\% \\ 
  Euro & 52.46\%/0.23\% & 43.17\%/0.05\% & 41.7\%/0.01\% \\ 
  Rice & 52.43\%/3.8\% & 45.81\%/1.3\% & 38.75\%/0.7\% \\ 
  BritishPound & 52.33\%/0.56\% & 47.66\%/0.17\% & 42.35\%/0.04\% \\ 
  VStoxxVolIdx & 50\%/0.08\% & 33.33\%/0.02\% & 22.22\%/0\% \\ 
  HeatingOil & 49.04\%/0.96\% & 37.31\%/0.44\% & 32.54\%/0.29\% \\ 
  Sugar & 48.51\%/1.15\% & 35.85\%/0.09\% & 28.43\%/0.05\% \\ 
  AusDollar & 47.99\%/1.79\% & 43.53\%/0.67\% & 41.21\%/0.24\% \\ 
  EURCHF & 47.91\%/0.03\% & 38.16\%/0.01\% & 29.86\%/0\% \\ 
  Lumber & 47.28\%/2.43\% & 39.81\%/0.65\% & 33.21\%/0.25\% \\ 
  Nikkei\_SIMX\_JPY & 46.97\%/0.12\% & 43.02\%/0.08\% & 39.25\%/0.07\% \\ 
  US10YNote & 46.94\%/0\% & 33.65\%/0\% & 26.41\%/0\% \\ 
  OMX25 & 46.32\%/0.85\% & 39.2\%/0.44\% & 34.22\%/0.29\% \\ 
  CnxNiftyIndia & 46.19\%/3\% & 40.41\%/2.33\% & 35.75\%/2.13\% \\ 
  Platinum & 45.89\%/3.87\% & 37.57\%/3.03\% & 33.29\%/2.65\% \\ 
  EuroDollar & 45.08\%/0.44\% & 38.72\%/0.13\% & 33.21\%/0.04\% \\ 
  Nikkei & 44.89\%/0.02\% & 35.79\%/0.01\% & 30.46\%/0.01\% \\ 
  CrudeOil & 44.09\%/0.26\% & 35.99\%/0.14\% & 31.55\%/0.1\% \\ 
  Russel2000 & 43.87\%/0.14\% & 37.16\%/0.12\% & 32.92\%/0.12\% \\ 
  US5Note & 43.7\%/0\% & 26.49\%/0\% & 19.8\%/0\% \\ 
  Peso & 42.23\%/0.02\% & 31.75\%/0\% & 24.44\%/0\% \\ 
  JapYen & 42.2\%/0.32\% & 35.52\%/0.07\% & 33.02\%/0.03\% \\ 
  LiveCattle & 41.74\%/0.73\% & 35.31\%/0.21\% & 28.99\%/0.08\% \\ 
  US2Note & 41.39\%/0\% & 35.68\%/0\% & 31.95\%/0\% \\ 
  CanDollar & 40.63\%/0.12\% & 33.46\%/0.05\% & 26.28\%/0.02\% \\ 
  Euribor & 40.31\%/0.35\% & 33.18\%/0.22\% & 28.34\%/0.15\% \\ 
  USTBond & 37.27\%/0.01\% & 30.8\%/0.01\% & 24.88\%/0\% \\ 
  miniSilver & 36.89\%/10.99\% & 33.17\%/10.83\% & 30.02\%/10.76\% \\ 
  SwissFranc & 36.49\%/0.16\% & 29.03\%/0.09\% & 24.35\%/0.04\% \\ 
  BEL20 & 36.16\%/0.3\% & 27.09\%/0.11\% & 20.56\%/0.04\% \\ 
  FeederCattle & 35.51\%/0.92\% & 27.09\%/0.2\% & 20.35\%/0.06\% \\ 
  EminiNasdaq & 35.31\%/0.09\% & 28.45\%/0.03\% & 22.37\%/0.01\% \\ 
  NatGas & 35.09\%/0.63\% & 27.4\%/0.17\% & 22.32\%/0.06\% \\ 
  Corn & 33.8\%/0.05\% & 29.04\%/0.02\% & 26.04\%/0.01\% \\ 
  MSCITaiwan & 32.11\%/0.11\% & 28.65\%/0.05\% & 26.86\%/0.03\% \\ 
  SoybeanOil & 31.86\%/0.51\% & 25.87\%/0.25\% & 21.17\%/0.17\% \\ 
  VIX & 31.64\%/0.14\% & 25.55\%/0.08\% & 21.63\%/0.06\% \\ 
  NZDollar & 31.5\%/0.1\% & 24.79\%/0.03\% & 20.3\%/0.01\% \\ 
  LeanHogs & 29.56\%/0.63\% & 21.97\%/0.19\% & 16.12\%/0.08\% \\ 
  SoybeanMeal & 24.19\%/0.41\% & 18.9\%/0.24\% & 15.11\%/0.19\% \\ 
  MSCISingapore & 24.12\%/0.18\% & 19.82\%/0.09\% & 17.75\%/0.07\% \\ 
  MDAX & 22.84\%/0.37\% & 14.98\%/0.16\% & 10.27\%/0.05\% \\ 
  Soybean & 21.71\%/0.11\% & 17.2\%/0.05\% & 14.78\%/0.03\% \\ 
  miniJGB & 21.66\%/0.24\% & 17.92\%/0.08\% & 15.16\%/0.04\% \\ 
  Wheat & 21.52\%/0.19\% & 16.51\%/0.08\% & 13.54\%/0.05\% \\ 
  SMI & 18.5\%/0.36\% & 16.52\%/0.59\% & 14.29\%/0.39\% \\ 
  EuroSwiss & 17.92\%/0.05\% & 13.89\%/0.02\% & 11.5\%/0.01\% \\ 
  ShortSterling & 15.43\%/0.01\% & 12.59\%/0.01\% & 10.69\%/0\% \\ 
   \hline
   \caption{Średnie wyniki dla współczynników TPR i FPR w zależności od mnożnika średniego spreadu. Wartości podano w postaci \emph{TPR/FPR}.}
\label{tab:result1}

\end{longtable}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{wykresy/ojfh10.PNG}
  \caption{Obserwacje odstające dla kontraktu na sok pomarańczowy. Kolorem cyjanowym zaznaczono obserwacje, które zostały oznaczone jako obserwacje odstające zarówno przez analityka, jak również filtr.}
  \label{fig:ojfh10}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{wykresy/fzwh11.PNG}
  \caption{Ścieżka obserwacji odstających na kontrakcie na pszenicę. Kolorem czerwonym zaznaczono obserwacje, które zostały oznaczone przez analityka jako odstające, w przypadku których filtr określił je jako poprawne. Należy zwrócić uwagę, że odstępy pomiędzy kolejnymi obserwacjami są niezbyt duże, dlatego też średni spread z każdą obserwacją powoli się rozszerza, co prowadzi do przepuszczenia praktycznie całej serii obserwacji odstających.}
  \label{fig:fzwh11}
\end{figure}

W tabelach \ref{tab:result2}, i \ref{tab:result3} zaprezentowano wybrane, surowe wyniki dotyczące kolejnych serii kontraktów terminowych na sok pomarańczowy i pszenicę. O ile w przypadku soku pomarańczowego filtr radził sobie dosyć dobrze (usuwał ok 82\% pojawiających się obserwacji odstających), to w przypadku pszenicy jakość działania filtra była dużo gorsza (w żadnym przypadku nie przekroczył 30\% usuniętych obserwacji odstających). Analiza wyników działania filtra pozwala więc sądzić, iż w przypadku soku pomarańczowego mamy raczej do czynienia z pojedynczymi obserwacjami odstającymi, natomiast dla pszenicy dominują serie tego typu obserwacji.

{\scriptsize
\begin{longtable}{lrllrrrrrr}
  \hline
Series & TickNo & TPR & FPR & Human & Filter & TP & FP & TN & FN \\ 
  \hline
  fOJ.F09 & 129368 & 92.33\% & 3.95\% & 2202 & 7145 & 2033 & 5112 & 122054 & 169 \\ 
  fOJ.H09 & 124745 & 97.91\% & 3.88\% & 2675 & 7464 & 2619 & 4845 & 117225 &  56 \\ 
  fOJ.K09 & 121813 & 75.57\% & 3.19\% & 3929 & 6853 & 2969 & 3884 & 114000 & 960 \\ 
  fOJ.N09 & 125683 & 97.79\% & 3.08\% & 1629 & 5459 & 1593 & 3866 & 120188 &  36 \\ 
  fOJ.U09 & 147274 & 92.37\% & 2.71\% & 2594 & 6381 & 2396 & 3985 & 140695 & 198 \\ 
  fOJ.X09 & 165710 & 87.67\% & 3.17\% & 3010 & 7898 & 2639 & 5259 & 157441 & 371 \\ 
  fOJ.F10 & 167922 & 98.28\% & 3.22\% & 2029 & 7395 & 1994 & 5401 & 160492 &  35 \\ 
  fOJ.H10 & 153285 & 94.34\% & 3.34\% & 1554 & 6588 & 1466 & 5122 & 146609 &  88 \\ 
  fOJ.K10 & 201806 & 94.9\% & 5.96\% & 3548 & 15386 & 3367 & 12019 & 186239 & 181 \\ 
  fOJ.N10 & 241420 & 88.94\% & 4.31\% & 3517 & 13523 & 3128 & 10395 & 227508 & 389 \\ 
  fOJ.U10 & 268211 & 92.27\% & 3.16\% & 4076 & 12238 & 3761 & 8477 & 255658 & 315 \\ 
  fOJ.X10 & 293368 & 95.13\% & 2.22\% & 3944 & 10279 & 3752 & 6527 & 282897 & 192 \\ 
  fOJ.F11 & 237612 & 91.96\% & 2.56\% & 1144 & 7144 & 1052 & 6092 & 230376 &  92 \\ 
  fOJ.H11 & 264521 & 92.83\% & 2.37\% & 1200 & 7370 & 1114 & 6256 & 257065 &  86 \\ 
  fOJ.K11 & 315312 & 91.1\% & 2.09\% & 1506 & 7962 & 1372 & 6590 & 307216 & 134 \\ 
  fOJ.N11 & 337454 & 14.97\% & 2.06\% & 13524 & 8960 & 2024 & 6936 & 316994 & 11500 \\ 
  fOJ.U11 & 343042 & 88.84\% & 2.22\% & 2706 & 10006 & 2404 & 7602 & 332734 & 302 \\ 
  fOJ.X11 & 371761 & 79.33\% & 3\% & 3121 & 13614 & 2476 & 11138 & 357502 & 645 \\ 
  fOJ.F12 & 367994 & 60.72\% & 2.92\% & 2678 & 12382 & 1626 & 10756 & 354560 & 1052 \\ 
  fOJ.H12 & 484820 & 90.86\% & 2.41\% & 1597 & 13152 & 1451 & 11701 & 471522 & 146 \\ 
  fOJ.K12 & 474927 & 84.96\% & 2.92\% & 1735 & 15333 & 1474 & 13859 & 459333 & 261 \\ 
  fOJ.N12 & 570145 & 66.34\% & 2.75\% & 4857 & 18928 & 3222 & 15706 & 549582 & 1635 \\ 
  fOJ.U12 & 590878 & 83.01\% & 1.7\% & 3397 & 12856 & 2820 & 10036 & 577445 & 577 \\ 
  fOJ.X12 & 593707 & 79.28\% & 1.84\% & 3075 & 13369 & 2438 & 10931 & 579701 & 637 \\ 
  fOJ.F13 & 547480 & 90.25\% & 3.85\% & 5517 & 26039 & 4979 & 21060 & 520903 & 538 \\ 
  fOJ.H13 & 530403 & 91.86\% & 1.97\% & 3085 & 13306 & 2834 & 10472 & 516846 & 251 \\ 
  fOJ.K13 & 626903 & 91.26\% & 1.84\% & 2928 & 14222 & 2672 & 11550 & 612425 & 256 \\ 
  fOJ.N13 & 644318 & 86.39\% & 1.54\% & 3783 & 13203 & 3268 & 9935 & 630600 & 515 \\ 
  fOJ.U13 & 609580 & 88.9\% & 3.73\% & 3569 & 25893 & 3173 & 22720 & 583291 & 396 \\ 
  fOJ.X13 & 590440 & 94.28\% & 1.35\% & 7358 & 14920 & 6937 & 7983 & 575099 & 421 \\ 
   \hline
\hline
\caption{Przedstawienie surowych wyników dla wybranych kontraktów na sok pomarańczowy (Orange Juice).}
\label{tab:result2}
\end{longtable}}

{\scriptsize
\begin{longtable}{lrllrrrrrr}
  \hline
Series & TickNo & TPR & FPR & Human & Filter & TP & FP & TN & FN \\ 
  \hline
fZW.H10 & 12937149 & 16.43\% & 0.05\% & 74144 & 19189 & 12181 & 7008 & 12855997 & 61963 \\ 
  fZW.K10 & 12527008 & 29.85\% & 0.04\% & 16170 & 10262 & 4826 & 5436 & 12505402 & 11344 \\ 
  fZW.N10 & 16426906 & 13.02\% & 0.13\% & 101145 & 34249 & 13173 & 21076 & 16304685 & 87972 \\ 
  fZW.U10 & 13382775 & 16.53\% & 0.05\% & 73828 & 19106 & 12204 & 6902 & 13302045 & 61624 \\ 
  fZW.Z10 & 21093397 & 14.83\% & 0.1\% & 121696 & 38868 & 18044 & 20824 & 20950877 & 103652 \\ 
  fZW.H11 & 17311228 & 11.77\% & 0.04\% & 123338 & 21080 & 14518 & 6562 & 17181328 & 108820 \\ 
  fZW.K11 & 16683574 & 25.39\% & 0.08\% & 36562 & 22218 & 9283 & 12935 & 16634077 & 27279 \\ 
  fZW.N11 & 20870618 & 12.13\% & 0.13\% & 168075 & 48014 & 20385 & 27629 & 20674914 & 147690 \\ 
  fZW.U11 & 17357599 & 21.34\% & 0.08\% & 72583 & 29023 & 15487 & 13536 & 17271480 & 57096 \\ 
  fZW.Z11 & 24211561 & 12.03\% & 0.05\% & 164041 & 32690 & 19726 & 12964 & 24034556 & 144315 \\ 
  fZW.H12 & 24136812 & 18.62\% & 0.07\% & 89056 & 32508 & 16582 & 15926 & 24031830 & 72474 \\ 
  fZW.K12 & 22541995 & 13.07\% & 0.07\% & 50426 & 21537 & 6590 & 14947 & 22476622 & 43836 \\ 
  fZW.N12 & 26012018 & 11.74\% & 0.35\% & 189249 & 113304 & 22211 & 91093 & 25731676 & 167038 \\ 
  fZW.U12 & 25896222 & 8.2\% & 0.1\% & 346158 & 53819 & 28368 & 25451 & 25524613 & 317790 \\ 
  fZW.Z12 & 31825007 & 9.31\% & 0.1\% & 268558 & 56178 & 25002 & 31176 & 31525273 & 243556 \\ 
  fZW.H13 & 33428075 & 11.48\% & 0.14\% & 534797 & 107876 & 61415 & 46461 & 32846817 & 473382 \\ 
  fZW.K13 & 30190592 & 3.67\% & 0.08\% & 931035 & 57371 & 34151 & 23220 & 29236337 & 896884 \\ 
  fZW.N13 & 26537769 & 8.31\% & 0.26\% & 196939 & 84703 & 16367 & 68336 & 26272494 & 180572 \\ 
  fZW.U13 & 16997608 & 28.5\% & 0.07\% & 203472 & 69463 & 57985 & 11478 & 16782658 & 145487 \\ 
  fZW.Z13 & 21169168 & 13.15\% & 0.08\% & 110940 & 32250 & 14586 & 17664 & 21040564 & 96354 \\ 
   \hline
\hline
\caption{Przedstawienie surowych wyników dla wybranych kontraktów na pszenicę (Wheat).}
\label{tab:result3}
\end{longtable}}



\chapter{Podsumowanie}

W pracy przedstawiono zagadnienie budowy rzetelnych danych finansowych, które mogą zostać użyte zarówno w testach algorytmicznej strategii inwestycyjnej, jak również w handlu rzeczywistym. Przedstawiono empiryczne obserwacje przemawiające za występowaniem obserwacji odstających w finansowych danych wysokiej częstotliwości, które mogą zaburzyć wszystkie procedury weryfikacji strategii. Również w tym kontekście została omówiona możliwość wykorzystania metod statystyki odpornej w handlu algorytmicznym do radzenia sobie z tego typu obserwacjami. Niestety mimo bardzo pożądanych własności statystycznych prezentowanych przez procedury odporne, często nie mogą one zostać wykorzystane z uwagi na zbyt dużą złożoność obliczeniową, jak również wymóg konsekwencji stosowania procedur odpornych w każdym module strategii (generowanie sygnałów, zarządzanie ryzykiem, etc.).

Przedstawiono również implementację prostego filtra usuwającego obserwacje odstające w oparciu o zachowanie spreadu między cenami Bid i Ask. Niestety prezentowany filtr tylko w niedużej części spełnił swoje zadanie, usuwając średnio około 50\% obserwacji odstających. Prezentowany wynik mimo, iż bardzo niski (przepuszczenie pojedynczej obserwacji odstającej jest bardzo niebezpieczne) pozwala sądzić, iż filtr może znaleźć zastosowanie w praktyce, jednak nie jako samodzielne narzędzie usuwające obserwacje odstające, ale część pewnego systemu filtrów. W dalszych pracach, by uzupełnić działanie filtra, należałoby zwrócić szczególną uwagę na wykrywanie poziomych i pionowych serii obserwacji odstających.


\bibliographystyle{plainZZ}
\bibliography{literatura_mag}
\addcontentsline{toc}{section}{Bibliografia}
\listoftables
\addcontentsline{toc}{section}{Spis tabel}
\listoffigures
\addcontentsline{toc}{section}{Spis rysunków}
\printindex
\end{document}
